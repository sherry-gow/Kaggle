{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a033c055",
   "metadata": {},
   "source": [
    "# Kaggle - Disaster Twitter - NLP Classification\n",
    "\n",
    "```\n",
    "Date:  2021-07-26\n",
    "Source: https://www.kaggle.com/fanglidayan/4-nlp-disaster-tweets/output\n",
    "Model: \n",
    "- tensorflow.keras \n",
    "\n",
    "Features:\n",
    "- text\n",
    "- hashtag\n",
    "- keyword\n",
    "- word embedding\n",
    "\n",
    "Libraries:\n",
    "- tensorflow.keras\n",
    "- nltk\n",
    "- spaCy\n",
    "- tweet-preprocessor\n",
    "- re\n",
    "- numpy\n",
    "- pandas\n",
    "\n",
    "Takeaways:\n",
    "- new feature engineering based on the hashtag\n",
    "- word embedding using spaCy en_core_web_lg pre-trained package\n",
    "- the word vector is stored in a numpy array (7613, 23, 300)\n",
    "- looks like the GRU layers are used with two middle-layer \n",
    "``` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5745dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\Disaster_Twitter_Yan.ipynb\n",
      ".\\preprocess_test.csv\n",
      ".\\preprocess_train.csv\n",
      ".\\sample_submission.csv\n",
      ".\\store_test.npy\n",
      ".\\store_train.npy\n",
      ".\\submission_nlp_tweets.csv\n",
      ".\\test.csv\n",
      ".\\train.csv\n",
      ".\\.ipynb_checkpoints\\Disaster_Twitter_Yan-checkpoint.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('.'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36956999",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv',index_col = 'id')\n",
    "test=pd.read_csv('test.csv',index_col = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "af509a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def isNaN(string):\n",
    "    return string != string\n",
    "isNaN('ablaze')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeec58ea",
   "metadata": {},
   "source": [
    "### lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "47c90aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text']=train['text'].apply(lambda x : x.lower())\n",
    "test['text']=test['text'].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "02140c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1        True\n",
       "4        True\n",
       "5        True\n",
       "6        True\n",
       "7        True\n",
       "         ... \n",
       "10869    True\n",
       "10870    True\n",
       "10871    True\n",
       "10872    True\n",
       "10873    True\n",
       "Name: keyword, Length: 7613, dtype: bool"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['keyword'].apply(lambda x: isNaN(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "dc97754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_keywords(keywords):\n",
    "    if keywords == keywords:\n",
    "        keywords=keywords.lower()\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "0ad7d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword']=train['keyword'].apply(lambda x : lower_keywords(x))\n",
    "test['keyword']=test['keyword'].apply(lambda x : lower_keywords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "13a8ba13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd wholesale markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>we always try to bring the heavy. #metal #rt h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#africanbaze: breaking news:nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword                       location  \\\n",
       "id                                          \n",
       "48  ablaze                     Birmingham   \n",
       "49  ablaze  Est. September 2012 - Bristol   \n",
       "50  ablaze                         AFRICA   \n",
       "\n",
       "                                                 text  target  \n",
       "id                                                             \n",
       "48  @bbcmtd wholesale markets ablaze http://t.co/l...       1  \n",
       "49  we always try to bring the heavy. #metal #rt h...       0  \n",
       "50  #africanbaze: breaking news:nigeria flag set a...       1  "
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[48:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9840b634",
   "metadata": {},
   "source": [
    "### extract hashtag and create new feature column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "66c38f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "train['hashtag'] = train['text'].apply(lambda x: re.findall(r'#(\\w+)', x))\n",
    "test['hashtag'] = test['text'].apply(lambda x: re.findall(r'#(\\w+)', x))\n",
    "# train['keyword'] = train['keyword'].apply(lambda x: re.findall(r'#(\\w+)', x) if not isNaN(x) else [])\n",
    "# test['keyword'] = test['keyword'].apply(lambda x: re.findall(r'#(\\w+)', x) if not isNaN(x) else [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "9db038b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd wholesale markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>we always try to bring the heavy. #metal #rt h...</td>\n",
       "      <td>0</td>\n",
       "      <td>[metal, rt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#africanbaze: breaking news:nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[africanbaze]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword                       location  \\\n",
       "id                                          \n",
       "48  ablaze                     Birmingham   \n",
       "49  ablaze  Est. September 2012 - Bristol   \n",
       "50  ablaze                         AFRICA   \n",
       "\n",
       "                                                 text  target        hashtag  \n",
       "id                                                                            \n",
       "48  @bbcmtd wholesale markets ablaze http://t.co/l...       1             []  \n",
       "49  we always try to bring the heavy. #metal #rt h...       0    [metal, rt]  \n",
       "50  #africanbaze: breaking news:nigeria flag set a...       1  [africanbaze]  "
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[48:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9445a6",
   "metadata": {},
   "source": [
    "###  remove digits (01234), urls (http://...), mentions (@...) and hashtags (#...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "4994f60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweet-preprocessor in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweet-preprocessor\n",
    "import preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "53b4ec13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package preprocessor:\n",
      "\n",
      "NAME\n",
      "    preprocessor\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    api\n",
      "    defines\n",
      "    enum\n",
      "    parse\n",
      "    preprocess\n",
      "    utils\n",
      "\n",
      "DATA\n",
      "    __all__ = [<function clean>, <function tokenize>, <function parse>, <f...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages\\preprocessor\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "d68264b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: preprocessor.clean(x))\n",
    "test['text'] = test['text'].apply(lambda x: preprocessor.clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "0af2463c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1    []\n",
       "Name: keyword, dtype: object"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[0:2]['keyword'].apply(lambda x: preprocessor.clean(x) if not isNaN(x) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "84891414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_list(lista):\n",
    "    try:\n",
    "        for i,ele in enumerate(lista):\n",
    "            lista[i]=preprocessor.clean(ele)\n",
    "        return lista\n",
    "    except:\n",
    "        print(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "7534897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hashtag'] = train['hashtag'].apply(lambda x: clear_list(x) if x is not None else [])\n",
    "test['hashtag'] = test['hashtag'].apply(lambda x: clear_list(x)  if x is not None else [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "19cf73ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>airplane%20accident</td>\n",
       "      <td>19.600858, -99.047821</td>\n",
       "      <td>experts in france begin examining airplane deb...</td>\n",
       "      <td>1</td>\n",
       "      <td>[news]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>airplane%20accident</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>strict liability in the context of an airplane...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>airplane%20accident</td>\n",
       "      <td>Salt Lake City, Utah</td>\n",
       "      <td>your lifetime odds of dying from an airplane a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>airplane%20accident</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>experts in france begin examining airplane deb...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 keyword               location  \\\n",
       "id                                                \n",
       "196  airplane%20accident  19.600858, -99.047821   \n",
       "197  airplane%20accident           Pennsylvania   \n",
       "198  airplane%20accident   Salt Lake City, Utah   \n",
       "199  airplane%20accident          Palo Alto, CA   \n",
       "\n",
       "                                                  text  target hashtag  \n",
       "id                                                                      \n",
       "196  experts in france begin examining airplane deb...       1  [news]  \n",
       "197  strict liability in the context of an airplane...       1      []  \n",
       "198  your lifetime odds of dying from an airplane a...       0      []  \n",
       "199  experts in france begin examining airplane deb...       1      []  "
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[196:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "157420d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here is a', 'is the web ui', 'this is special %435 how']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clear_list(['here is a #hashtag', 'https://google.com is the web ui', 'this is special %435 how'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0bb9c4",
   "metadata": {},
   "source": [
    "###  recover abbreviations (change they'll to they will, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "2246de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x: decontracted(x))\n",
    "test['text'] = test['text'].apply(lambda x: decontracted(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fa6a4b",
   "metadata": {},
   "source": [
    "### remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "d581fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(lista):\n",
    "    for i,ele in enumerate(lista):\n",
    "        lista[i] = re.sub(r'[^\\w\\s]', '', ele)\n",
    "        lista[i] = re.sub('_', ' ', lista[i]) # the previous row doesn't remove underscore\n",
    "    return lista\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "6b0a0e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text']=train['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "train['text']=train['text'].apply(lambda x: re.sub('_', ' ', x))\n",
    "\n",
    "test['text']=test['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "test['text']=test['text'].apply(lambda x: re.sub('_', ' ', x)) # the previous row doesn't remove \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "bd0aa0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Live On Webcam</td>\n",
       "      <td>check these out</td>\n",
       "      <td>0</td>\n",
       "      <td>[nsfw]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>England.</td>\n",
       "      <td>first night with retainers in it is quite weir...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Sheffield Township, Ohio</td>\n",
       "      <td>deputies man shot before brighton home set ablaze</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>India</td>\n",
       "      <td>man wife get six years jail for setting ablaze...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Barbados</td>\n",
       "      <td>santa cruz head of the st elizabeth police sup...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Anaheim</td>\n",
       "      <td>police arsonist deliberately set black church ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Abuja</td>\n",
       "      <td>noches elbestia  happy to see my teammates and...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>USA</td>\n",
       "      <td>trampling on turkmen flag later set it ablaze ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[kurds, diyala]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>truck ablaze  r21 voortrekker ave outside or t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Sao Paulo, Brazil</td>\n",
       "      <td>set our hearts ablaze and every city was a gif...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>hollywoodland</td>\n",
       "      <td>they sky was ablaze tonight in los angeles i a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Edmonton, Alberta - Treaty 6</td>\n",
       "      <td>how the west was burned thousands of wildfires...</td>\n",
       "      <td>1</td>\n",
       "      <td>[california, climate, energy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword                      location  \\\n",
       "id                                         \n",
       "68  ablaze                Live On Webcam   \n",
       "71  ablaze                      England.   \n",
       "73  ablaze      Sheffield Township, Ohio   \n",
       "74  ablaze                         India   \n",
       "76  ablaze                      Barbados   \n",
       "77  ablaze                       Anaheim   \n",
       "78  ablaze                         Abuja   \n",
       "79  ablaze                           USA   \n",
       "80  ablaze                  South Africa   \n",
       "81  ablaze             Sao Paulo, Brazil   \n",
       "82  ablaze                hollywoodland    \n",
       "83  ablaze  Edmonton, Alberta - Treaty 6   \n",
       "\n",
       "                                                 text  target  \\\n",
       "id                                                              \n",
       "68                                    check these out       0   \n",
       "71  first night with retainers in it is quite weir...       0   \n",
       "73  deputies man shot before brighton home set ablaze       1   \n",
       "74  man wife get six years jail for setting ablaze...       1   \n",
       "76  santa cruz head of the st elizabeth police sup...       0   \n",
       "77  police arsonist deliberately set black church ...       1   \n",
       "78  noches elbestia  happy to see my teammates and...       0   \n",
       "79  trampling on turkmen flag later set it ablaze ...       1   \n",
       "80  truck ablaze  r21 voortrekker ave outside or t...       1   \n",
       "81  set our hearts ablaze and every city was a gif...       0   \n",
       "82  they sky was ablaze tonight in los angeles i a...       0   \n",
       "83  how the west was burned thousands of wildfires...       1   \n",
       "\n",
       "                          hashtag  \n",
       "id                                 \n",
       "68                         [nsfw]  \n",
       "71                             []  \n",
       "73                             []  \n",
       "74                             []  \n",
       "76                             []  \n",
       "77                             []  \n",
       "78                             []  \n",
       "79                [kurds, diyala]  \n",
       "80                             []  \n",
       "81                             []  \n",
       "82                             []  \n",
       "83  [california, climate, energy]  "
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[48:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "d6b2c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hashtag']=train['hashtag'].apply(lambda x: remove_punc(x))\n",
    "test['hashtag']=test['hashtag'].apply(lambda x: remove_punc(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe7f4a",
   "metadata": {},
   "source": [
    "### remove digits from keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "c0291734",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword']=train['keyword'].apply(lambda x: re.sub(r'[^\\D]', ' ', x) if not isNaN(x) else '')\n",
    "test['keyword']=test['keyword'].apply(lambda x: re.sub(r'[^\\D]', ' ', x)if not isNaN(x) else '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "55eb5b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>airplane  accident</td>\n",
       "      <td>19.600858, -99.047821</td>\n",
       "      <td>experts in france begin examining airplane deb...</td>\n",
       "      <td>1</td>\n",
       "      <td>[news]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>airplane  accident</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>strict liability in the context of an airplane...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>airplane  accident</td>\n",
       "      <td>Salt Lake City, Utah</td>\n",
       "      <td>your lifetime odds of dying from an airplane a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>airplane  accident</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>experts in france begin examining airplane deb...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                keyword               location  \\\n",
       "id                                               \n",
       "196  airplane  accident  19.600858, -99.047821   \n",
       "197  airplane  accident           Pennsylvania   \n",
       "198  airplane  accident   Salt Lake City, Utah   \n",
       "199  airplane  accident          Palo Alto, CA   \n",
       "\n",
       "                                                  text  target hashtag  \n",
       "id                                                                      \n",
       "196  experts in france begin examining airplane deb...       1  [news]  \n",
       "197  strict liability in the context of an airplane...       1      []  \n",
       "198  your lifetime odds of dying from an airplane a...       0      []  \n",
       "199  experts in france begin examining airplane deb...       1      []  "
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[196:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "32b7a584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'air  balx'"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'[^\\D]', ' ', 'air20balx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce77e00",
   "metadata": {},
   "source": [
    "### tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "47e90843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sherry.gow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "34162bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text']=train['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "test['text']=test['text'].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "56257fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword']=train['keyword'].apply(lambda x: nltk.word_tokenize(x))\n",
    "test['keyword']=test['keyword'].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "f6e28780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>[airplane, accident]</td>\n",
       "      <td>19.600858, -99.047821</td>\n",
       "      <td>[experts, in, france, begin, examining, airpla...</td>\n",
       "      <td>1</td>\n",
       "      <td>[news]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>[airplane, accident]</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[strict, liability, in, the, context, of, an, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>[airplane, accident]</td>\n",
       "      <td>Salt Lake City, Utah</td>\n",
       "      <td>[your, lifetime, odds, of, dying, from, an, ai...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>[airplane, accident]</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>[experts, in, france, begin, examining, airpla...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  keyword               location  \\\n",
       "id                                                 \n",
       "196  [airplane, accident]  19.600858, -99.047821   \n",
       "197  [airplane, accident]           Pennsylvania   \n",
       "198  [airplane, accident]   Salt Lake City, Utah   \n",
       "199  [airplane, accident]          Palo Alto, CA   \n",
       "\n",
       "                                                  text  target hashtag  \n",
       "id                                                                      \n",
       "196  [experts, in, france, begin, examining, airpla...       1  [news]  \n",
       "197  [strict, liability, in, the, context, of, an, ...       1      []  \n",
       "198  [your, lifetime, odds, of, dying, from, an, ai...       0      []  \n",
       "199  [experts, in, france, begin, examining, airpla...       1      []  "
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[196:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07631f5f",
   "metadata": {},
   "source": [
    "###  remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "4a84e7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sherry.gow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "f80c97d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('english')\n",
    "stop_words.append('u') # 'i love u' is the semantically the same as 'i love you'\n",
    "stop_words.append('one') # want to remove numbers\n",
    "stop_words.append('two')\n",
    "stop_words.append('three')\n",
    "stop_words.append('four')\n",
    "stop_words.append('five')\n",
    "stop_words.append('six')\n",
    "stop_words.append('seven')\n",
    "stop_words.append('eight')\n",
    "stop_words.append('nine')\n",
    "stop_words.append('ten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "fc32687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(lista):\n",
    "    return [i for i in lista if i not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "5ef52b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[happened, terrible, car, crash]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[heard, different, cities, stay, safe, everyone]</td>\n",
       "      <td>[earthquake]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[forest, fire, spot, pond, geese, fleeing, acr...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[apocalypse, lighting]</td>\n",
       "      <td>[spokane, wildfires]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[typhoon, soudelor, kills, china, taiwan]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10861</th>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[earthquake, safety, los, angeles, safety, fas...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10865</th>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[storm, ri, worse, last, hurricane, cityamp3ot...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10868</th>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[green, line, derailment, chicago]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[meg, issues, hazardous, weather, outlook, hwo]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[activated, municipal, emergency, plan]</td>\n",
       "      <td>[cityofcalgary, yycstorm]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      keyword location                                               text  \\\n",
       "id                                                                          \n",
       "0          []      NaN                   [happened, terrible, car, crash]   \n",
       "2          []      NaN   [heard, different, cities, stay, safe, everyone]   \n",
       "3          []      NaN  [forest, fire, spot, pond, geese, fleeing, acr...   \n",
       "9          []      NaN                             [apocalypse, lighting]   \n",
       "11         []      NaN          [typhoon, soudelor, kills, china, taiwan]   \n",
       "...       ...      ...                                                ...   \n",
       "10861      []      NaN  [earthquake, safety, los, angeles, safety, fas...   \n",
       "10865      []      NaN  [storm, ri, worse, last, hurricane, cityamp3ot...   \n",
       "10868      []      NaN                 [green, line, derailment, chicago]   \n",
       "10874      []      NaN    [meg, issues, hazardous, weather, outlook, hwo]   \n",
       "10875      []      NaN            [activated, municipal, emergency, plan]   \n",
       "\n",
       "                         hashtag  \n",
       "id                                \n",
       "0                             []  \n",
       "2                   [earthquake]  \n",
       "3                             []  \n",
       "9           [spokane, wildfires]  \n",
       "11                            []  \n",
       "...                          ...  \n",
       "10861                         []  \n",
       "10865                         []  \n",
       "10868                         []  \n",
       "10874                         []  \n",
       "10875  [cityofcalgary, yycstorm]  \n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst =['text', 'hashtag', 'keyword']\n",
    "for l in lst:\n",
    "    train[l]=train[l].apply(lambda x:  remove_stop_words(x) if x is not None else [] )\n",
    "    test[l]=test[l].apply(lambda x: remove_stop_words(x) if x is not None else [])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "bd9caf59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>[airplane, accident]</td>\n",
       "      <td>19.600858, -99.047821</td>\n",
       "      <td>[experts, france, begin, examining, airplane, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[news]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>[airplane, accident]</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>[strict, liability, context, airplane, acciden...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>[airplane, accident]</td>\n",
       "      <td>Salt Lake City, Utah</td>\n",
       "      <td>[lifetime, odds, dying, airplane, accident]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>[airplane, accident]</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>[experts, france, begin, examining, airplane, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  keyword               location  \\\n",
       "id                                                 \n",
       "196  [airplane, accident]  19.600858, -99.047821   \n",
       "197  [airplane, accident]           Pennsylvania   \n",
       "198  [airplane, accident]   Salt Lake City, Utah   \n",
       "199  [airplane, accident]          Palo Alto, CA   \n",
       "\n",
       "                                                  text  target hashtag  \n",
       "id                                                                      \n",
       "196  [experts, france, begin, examining, airplane, ...       1  [news]  \n",
       "197  [strict, liability, context, airplane, acciden...       1      []  \n",
       "198        [lifetime, odds, dying, airplane, accident]       0      []  \n",
       "199  [experts, france, begin, examining, airplane, ...       1      []  "
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[196:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51916585",
   "metadata": {},
   "source": [
    "### lemmatization. ('us' is lemmatized to 'u')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ef8966f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sherry.gow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "a7f10f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_list(lista):\n",
    "    return [WordNetLemmatizer().lemmatize(i) for i in lista ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "6dc13c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in lst:\n",
    "    train[l] = train[l].apply(lambda x: lemmatize_list(x) if x is not None else [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b58b37",
   "metadata": {},
   "source": [
    "### save preprocess file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "5b541b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('preprocess_train.csv', index=False)\n",
    "test.to_csv('preprocess_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6b5808",
   "metadata": {},
   "source": [
    "### Find max text, hashtag and keyword length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "e712261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_train =[]\n",
    "max_len_test=[]\n",
    "for l in lst:\n",
    "    max_len_train.append(train[l].apply(lambda x: len(x)).max())\n",
    "    max_len_test.append(test[l].apply(lambda x: len(x)).max())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "57591f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 13, 2]"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f60ddc",
   "metadata": {},
   "source": [
    "### install and understand word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b2bda4fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_lg==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.3.1/en_core_web_lg-2.3.1.tar.gz (782.7 MB)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\sherry.gow\\appdata\\roaming\\python\\python38\\site-packages (from en_core_web_lg==2.3.1) (2.3.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sherry.gow\\appdata\\roaming\\python\\python38\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.19.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\sherry.gow\\appdata\\roaming\\python\\python38\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (4.59.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sherry.gow\\appdata\\roaming\\python\\python38\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\sherry.gow\\appdata\\roaming\\python\\python38\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (0.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sherry.gow\\appdata\\roaming\\python\\python38\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\sherry.gow\\appdata\\roaming\\python\\python38\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (52.0.0.post20210125)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\sherry.gow\\appdata\\roaming\\python\\python38\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\sherry.gow\\appdata\\roaming\\python\\python38\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\sherry.gow\\appdata\\roaming\\python\\python38\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.26.4)\n",
      "Building wheels for collected packages: en-core-web-lg\n",
      "  Building wheel for en-core-web-lg (setup.py): started\n",
      "  Building wheel for en-core-web-lg (setup.py): still running...\n",
      "  Building wheel for en-core-web-lg (setup.py): finished with status 'done'\n",
      "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.3.1-py3-none-any.whl size=782936124 sha256=b218d0332c3a45346f68bccce960c21dceee305e0e120b9d95d212b74541ff53\n",
      "  Stored in directory: c:\\users\\sherry.gow\\appdata\\local\\pip\\cache\\wheels\\8b\\bb\\bb\\bdc918f4b37d930a1be9ed876e7b2c2ee518a34803d78a248e\n",
      "Successfully built en-core-web-lg\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-2.3.1\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f8bc7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "22ad22f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"mom dad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f62bae55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.28115013e-02,  3.05620015e-01, -3.15519989e-01,  3.11255008e-01,\n",
       "        8.75004977e-02, -4.12320010e-02,  3.44820023e-01, -8.56235027e-01,\n",
       "        3.60069990e-01,  2.51300001e+00, -4.11954999e-01,  3.99410009e-01,\n",
       "        8.68709981e-02, -1.40648499e-01, -4.97725010e-01,  1.14992499e-01,\n",
       "        7.12064952e-02,  2.59263009e-01,  1.75907493e-01,  4.31775004e-01,\n",
       "       -1.37494996e-01, -6.74854994e-01,  3.62884998e-02, -3.31864990e-02,\n",
       "        6.85489997e-02,  1.65477484e-01, -3.04780006e-01, -2.54519999e-01,\n",
       "       -2.12589994e-01, -5.08745015e-01, -1.91990003e-01,  9.50000249e-04,\n",
       "        3.19710016e-01,  5.07709011e-02,  5.62629998e-01,  9.37424973e-02,\n",
       "        3.72875005e-01, -1.54286996e-01,  1.79989990e-02,  2.53154989e-02,\n",
       "       -1.46919996e-01, -3.45575005e-01,  9.03474987e-02,  4.14749980e-02,\n",
       "       -1.22122504e-01,  2.38005012e-01, -1.83737487e-01, -5.23784995e-01,\n",
       "        6.53060019e-01,  2.85200030e-02,  9.54950005e-02, -2.43465006e-01,\n",
       "        3.75999957e-02, -2.26395018e-02, -3.56649995e-01,  1.52675003e-01,\n",
       "       -1.12356998e-01, -3.89285013e-02,  2.53479987e-01,  4.60750014e-02,\n",
       "       -7.15501606e-04,  8.71000439e-03, -1.45502999e-01, -3.29337008e-02,\n",
       "        3.90349984e-01, -4.31824982e-01,  2.93630004e-01,  5.30390024e-01,\n",
       "        4.92300019e-02,  9.67000052e-03, -3.48140001e-01, -3.35824996e-01,\n",
       "       -2.61079997e-01,  2.37159997e-01,  4.24010009e-02,  4.49654996e-01,\n",
       "        1.67746499e-01,  3.98509979e-01, -1.96680009e-01,  8.18395019e-02,\n",
       "        5.31740010e-01,  4.05385017e-01, -2.22194999e-01,  3.23980004e-02,\n",
       "        3.79099995e-01, -5.27220011e-01,  1.01732004e+00, -1.37389496e-01,\n",
       "        2.25285009e-01,  1.11039504e-01, -5.03435016e-01,  3.07965010e-01,\n",
       "       -6.84730038e-02,  1.74060002e-01,  1.01925507e-01,  9.36864913e-02,\n",
       "        5.82029969e-02, -9.21635032e-02,  2.33785007e-02, -1.82560012e-02,\n",
       "        8.46709982e-02, -7.38589931e-03,  4.00285006e-01, -1.30649492e-01,\n",
       "       -5.30304983e-02,  5.18615022e-02,  1.22853503e-01,  7.03414977e-02,\n",
       "        2.08548486e-01,  1.00356996e-01, -2.10364997e-01,  9.62354988e-02,\n",
       "       -3.19615006e-01,  4.95344996e-02,  5.08809984e-02, -1.08939499e-01,\n",
       "        3.18659991e-01, -2.48750001e-02,  1.45479992e-01,  3.76450002e-01,\n",
       "        1.29730001e-01, -3.91840003e-02,  9.02649984e-02, -2.81545013e-01,\n",
       "        3.15970004e-01,  2.71640003e-01, -2.06615001e-01, -3.04799974e-01,\n",
       "       -1.99355006e-01, -1.67345002e-01, -2.47900009e-01,  2.32820496e-01,\n",
       "       -2.43184999e-01,  7.55465031e-02, -8.10154974e-02, -3.69080007e-01,\n",
       "        3.74750011e-02,  3.53009999e-01, -1.73559994e-01,  4.05264974e-01,\n",
       "       -3.02290010e+00, -1.26370996e-01,  3.72409999e-01,  2.46755004e-01,\n",
       "       -7.15345025e-01,  3.37750018e-01, -5.34115016e-01, -3.26220021e-02,\n",
       "       -1.28059000e-01, -1.79170012e-01, -3.67190003e-01,  1.30820498e-01,\n",
       "       -4.74799991e-01, -1.59749508e-01, -3.08099985e-02, -4.17620003e-01,\n",
       "       -1.34735003e-01, -1.26250997e-01, -7.83020034e-02, -4.43580002e-01,\n",
       "       -1.74827501e-01, -7.71639943e-02, -2.70080000e-01, -2.48324990e-01,\n",
       "       -2.77469993e-01, -5.45145035e-01, -1.68945000e-01,  7.29815960e-02,\n",
       "        3.95205021e-01,  8.04475024e-02, -2.84745008e-01, -1.76116005e-01,\n",
       "        3.89654994e-01,  1.92375004e-01,  5.05574197e-02, -2.37379998e-01,\n",
       "       -2.75260001e-01,  4.17944998e-01, -4.46385026e-01,  2.34990001e-01,\n",
       "       -2.85099987e-02, -4.73514974e-01, -4.47194993e-01,  2.35619992e-01,\n",
       "       -8.98949802e-03,  1.46738991e-01,  4.34116498e-02,  1.50444999e-01,\n",
       "       -1.69045001e-01, -3.16684991e-01, -2.59804994e-01, -1.78925008e-01,\n",
       "       -6.28120005e-01, -2.33940005e-01,  3.79440002e-02, -2.17455000e-01,\n",
       "        4.86665010e-01, -1.37419000e-01,  2.72920012e-01, -1.96525007e-01,\n",
       "       -4.14645016e-01, -1.32408500e-01, -1.16429999e-02, -2.49769986e-01,\n",
       "       -2.92530000e-01,  2.02115010e-02,  1.92690000e-01, -6.74735010e-02,\n",
       "        3.63185018e-01, -3.51334989e-01, -5.13660014e-01, -2.85050005e-01,\n",
       "       -1.79140002e-01,  4.23309982e-01, -1.52795002e-01, -2.68355012e-01,\n",
       "       -1.08055495e-01, -1.95600003e-01, -1.84230000e-01, -2.10565001e-01,\n",
       "       -2.25759998e-01, -3.66919994e-01, -1.03129998e-01, -8.34404975e-02,\n",
       "       -5.30700013e-02,  1.54740006e-01,  2.16754004e-01,  1.79899991e-01,\n",
       "       -1.90625012e-01, -1.52620003e-01, -2.83219993e-01, -2.28560001e-01,\n",
       "        4.05915022e-01,  3.26795012e-01, -1.78822987e-02, -2.46910006e-01,\n",
       "        1.46510005e-01,  1.63698494e-01,  2.81354994e-01, -3.55005264e-04,\n",
       "       -2.59315014e-01,  3.95534970e-02, -1.09655008e-01,  4.15809989e-01,\n",
       "        1.49774998e-01,  1.35265008e-01, -1.96354002e-01, -3.29585016e-01,\n",
       "        1.10238999e-01,  1.35394990e-01,  6.79740012e-02, -5.37799969e-02,\n",
       "       -2.49255020e-02,  1.00779995e-01,  1.34509504e-01,  6.91400021e-02,\n",
       "       -4.66904998e-01, -1.42714500e-01, -3.93454999e-01,  9.61449929e-04,\n",
       "       -4.18919981e-01,  2.81100005e-01, -2.32924998e-01, -2.66034991e-01,\n",
       "       -1.39485508e-01,  4.32150006e-01, -3.12734991e-01,  9.81630012e-02,\n",
       "       -7.83830047e-01,  2.81839997e-01, -4.63225007e-01, -1.84395000e-01,\n",
       "       -1.30147010e-01,  9.40300003e-02, -4.26895022e-01, -2.05930006e-02,\n",
       "       -3.70539993e-01,  1.16619498e-01, -6.51770011e-02,  9.29000229e-03,\n",
       "        1.40915006e-01,  2.54759997e-01,  2.18276501e-01, -2.42379993e-01,\n",
       "        1.55542001e-01,  3.05604994e-01,  2.03810006e-01, -3.22709978e-02,\n",
       "       -5.16695023e-01,  5.46660006e-01,  5.17910004e-01, -1.56656876e-01,\n",
       "       -1.14144981e-02,  2.93730021e-01,  4.83599991e-01, -6.96199983e-02,\n",
       "        2.28875011e-01,  2.10274503e-01, -2.48005018e-02, -1.26727000e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b4610790",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8566172"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].similarity(doc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a03ae3",
   "metadata": {},
   "source": [
    "### use word embedding to create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "1435bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m - documents length = 7613\n",
    "# n max of  - hard coded to 23 + 13 + 2 = 38\n",
    "# r - hard coded to 300\n",
    "# init a numpy with dimention (m,n,r) - (7613, 38, 300)\n",
    "m=train.shape[0]\n",
    "n = 0\n",
    "for i in max_len_train:\n",
    "    n = n+i    \n",
    "r = 300\n",
    "store_train=np.zeros((m,n,r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "5b56bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index()\n",
    "test=test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "a1c9f827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "199\n",
      "299\n",
      "399\n",
      "499\n",
      "599\n",
      "699\n",
      "799\n",
      "899\n",
      "999\n",
      "1099\n",
      "1199\n",
      "1299\n",
      "1399\n",
      "1499\n",
      "1599\n",
      "1699\n",
      "1799\n",
      "1899\n",
      "1999\n",
      "2099\n",
      "2199\n",
      "2299\n",
      "2399\n",
      "2499\n",
      "2599\n",
      "2699\n",
      "2799\n",
      "2899\n",
      "2999\n",
      "3099\n",
      "3199\n",
      "3299\n",
      "3399\n",
      "3499\n",
      "3599\n",
      "3699\n",
      "3799\n",
      "3899\n",
      "3999\n",
      "4099\n",
      "4199\n",
      "4299\n",
      "4399\n",
      "4499\n",
      "4599\n",
      "4699\n",
      "4799\n",
      "4899\n",
      "4999\n",
      "5099\n",
      "5199\n",
      "5299\n",
      "5399\n",
      "5499\n",
      "5599\n",
      "5699\n",
      "5799\n",
      "5899\n",
      "5999\n",
      "6099\n",
      "6199\n",
      "6299\n",
      "6399\n",
      "6499\n",
      "6599\n",
      "6699\n",
      "6799\n",
      "6899\n",
      "6999\n",
      "7099\n",
      "7199\n",
      "7299\n",
      "7399\n",
      "7499\n",
      "7599\n"
     ]
    }
   ],
   "source": [
    "for i in range(m): # m\n",
    "    if i % 100 == 99:\n",
    "        print(i)\n",
    "    for j in range(len(train['text'][i])): # length of the list ['love','peace','compassion','wisdom']        \n",
    "        store_train[i,j,:]=nlp(train['text'][i][j])[0].vector\n",
    "    for j in range(len(train['hashtag'][i])):\n",
    "        try:\n",
    "            store_train[i,23+j,:]=nlp(train['hashtag'][i][j])[0].vector\n",
    "        except:\n",
    "            store_train[i,23+j,:]=nlp(train['hashtag'][i][j]).vector # in the case when hashtag is [''] instead of ['some','word']\n",
    "    for j in range(len(train['keyword'][i])):\n",
    "        store_train[i,36+j,:]=nlp(train['keyword'][i][j])[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "3290bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('store_train.npy', store_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "b3185d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "642dd1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "199\n",
      "299\n",
      "399\n",
      "499\n",
      "599\n",
      "699\n",
      "799\n",
      "899\n",
      "999\n",
      "1099\n",
      "1199\n",
      "1299\n",
      "1399\n",
      "1499\n",
      "1599\n",
      "1699\n",
      "1799\n",
      "1899\n",
      "1999\n",
      "2099\n",
      "2199\n",
      "2299\n",
      "2399\n",
      "2499\n",
      "2599\n",
      "2699\n",
      "2799\n",
      "2899\n",
      "2999\n",
      "3099\n",
      "3199\n"
     ]
    }
   ],
   "source": [
    "m=test.shape[0]\n",
    "store_test=np.zeros((m,n,r))\n",
    "for i in range(m): # m\n",
    "    if i % 100 == 99:\n",
    "        print(i)\n",
    "    for j in range(len(test['text'][i])): # length of the list ['love','peace','compassion','wisdom']        \n",
    "        store_test[i,j,:]=nlp(test['text'][i][j])[0].vector\n",
    "    for j in range(len(test['hashtag'][i])):\n",
    "        try:\n",
    "            store_test[i,23+j,:]=nlp(test['hashtag'][i][j])[0].vector\n",
    "        except:\n",
    "            store_test[i,23+j,:]=nlp(test['hashtag'][i][j]).vector # in the case when hashtag is [''] instead of ['some','word']\n",
    "    for j in range(len(test['keyword'][i])):\n",
    "        store_test[i,36+j,:]=nlp(test['keyword'][i][j])[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "a70d9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('store_test.npy',store_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15be62",
   "metadata": {},
   "source": [
    "### slice the train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "f5b65d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 23, 300)\n",
      "(7613, 13, 300)\n",
      "(7613, 2, 300)\n"
     ]
    }
   ],
   "source": [
    "maxi_text = max_len_train[0]\n",
    "maxi_hashtag = max_len_train[1]\n",
    "maxi_keyword = max_len_train[2]\n",
    "\n",
    "\n",
    "store_train_text = store_train[:, :maxi_text, :]\n",
    "store_train_hashtag=store_train[:,maxi_text:maxi_text+maxi_hashtag,:]\n",
    "store_train_keyword=store_train[:,-maxi_keyword:,:]\n",
    "\n",
    "print(store_train_text.shape)\n",
    "print(store_train_hashtag.shape)\n",
    "print(store_train_keyword.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "66335dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_test_text=store_train[:,:maxi_text,:]\n",
    "store_test_hashtag=store_train[:,maxi_text:maxi_text+maxi_hashtag,:]\n",
    "store_test_keyword=store_train[:,-maxi_keyword:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65cbd96",
   "metadata": {},
   "source": [
    "### modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "3dabe0f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.5.0-cp38-cp38-win_amd64.whl (422.6 MB)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting tensorboard~=2.5\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp38-cp38-win_amd64.whl (2.9 MB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.5)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\sherry.gow\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow) (3.17.3)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from tensorflow) (0.36.2)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (2.25.1)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.33.1-py2.py3-none-any.whl (152 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2020.12.5)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=2e3bcabba8c550777c5c99a3b4cd75fdd3d0edbd7a97856d83baef3f8931a98d\n",
      "  Stored in directory: c:\\users\\sherry.gow\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras-nightly, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 cachetools-4.2.2 flatbuffers-1.12 gast-0.4.0 google-auth-1.33.1 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.34.1 h5py-3.1.0 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.4 oauthlib-3.1.1 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "4925c016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: h5py in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from keras) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from keras) (1.19.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages (from keras) (1.5.4)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "712c01eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as L\n",
    "# from tensorflow.keras.layers import Input, Dropout, GRU, BatchNormalization, TimeDistributed, Reshapre, Dense, Conv1D, Concatenate\n",
    "import tensorflow.keras.models as M \n",
    "import tensorflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "920ea26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=Input(shape=(store_train.shape[1],store_train_text.shape[2]))\n",
    "\n",
    "mid=L.GRU(units=300, return_sequences=True)(inp)\n",
    "mid=L.Dropout(0.6)(mid)\n",
    "mid=L.BatchNormalization()(mid)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "abbdf176",
   "metadata": {},
   "outputs": [],
   "source": [
    "mid=L.GRU(units=300, return_sequences=True)(mid)\n",
    "mid=L.Dropout(0.6)(mid)\n",
    "mid=L.BatchNormalization()(mid)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "0aefbfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mid=L.GRU(units=300, return_sequences=True)(mid)\n",
    "mid=L.Dropout(0.6)(mid)\n",
    "mid=L.BatchNormalization()(mid)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "66a7e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "mid=L.Dropout(0.6)(mid)\n",
    "mid=L.TimeDistributed(L.Dense(1,activation='relu'))(mid)\n",
    "mid=L.Reshape((mid.shape[1],))(mid)\n",
    "mid=L.Dropout(0.6)(mid)\n",
    "mid=L.BatchNormalization()(mid) \n",
    "outp=L.Dense(2,activation='softmax')(mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "b40cec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=M.Model(inputs=inp, outputs=outp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "4dcb6ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x1d714618790>"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "f8d8a0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tensorflow.keras.models in tensorflow.keras:\n",
      "\n",
      "NAME\n",
      "    tensorflow.keras.models - Code for model cloning, plus model-related API entries.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "\n",
      "\n",
      "FILE\n",
      "    c:\\users\\sherry.gow\\anaconda3\\lib\\site-packages\\tensorflow\\keras\\models\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1bbef2",
   "metadata": {},
   "source": [
    "### create labels for the training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "44c42d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_train=np.load('store_train.npy')\n",
    "\n",
    "m=store_train.shape[0]\n",
    "train_Y=np.zeros((m,2))\n",
    "for i in range(m):\n",
    "    train_Y[i,train.iloc[i]['target']]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "0c64b283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "cddfb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sed=13\n",
    "np.random.seed(sed)\n",
    "np.random.shuffle(store_train)\n",
    "np.random.seed(sed)\n",
    "np.random.shuffle(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "0d0e5b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy',metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "ffef9a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "112/112 [==============================] - 117s 989ms/step - loss: 0.7755 - accuracy: 0.6412\n",
      "Epoch 2/50\n",
      "112/112 [==============================] - 107s 954ms/step - loss: 0.5895 - accuracy: 0.7285\n",
      "Epoch 3/50\n",
      "112/112 [==============================] - 110s 985ms/step - loss: 0.5313 - accuracy: 0.7658\n",
      "Epoch 4/50\n",
      "112/112 [==============================] - 107s 955ms/step - loss: 0.5093 - accuracy: 0.7755\n",
      "Epoch 5/50\n",
      "112/112 [==============================] - 109s 976ms/step - loss: 0.4804 - accuracy: 0.7929\n",
      "Epoch 6/50\n",
      "112/112 [==============================] - 106s 948ms/step - loss: 0.4558 - accuracy: 0.8126\n",
      "Epoch 7/50\n",
      "112/112 [==============================] - 108s 965ms/step - loss: 0.4431 - accuracy: 0.8140\n",
      "Epoch 8/50\n",
      "112/112 [==============================] - 107s 957ms/step - loss: 0.4331 - accuracy: 0.8261\n",
      "Epoch 9/50\n",
      "112/112 [==============================] - 107s 951ms/step - loss: 0.4094 - accuracy: 0.8420\n",
      "Epoch 10/50\n",
      "112/112 [==============================] - 108s 965ms/step - loss: 0.4171 - accuracy: 0.8344\n",
      "Epoch 11/50\n",
      "112/112 [==============================] - 108s 966ms/step - loss: 0.3831 - accuracy: 0.8510\n",
      "Epoch 12/50\n",
      "112/112 [==============================] - 108s 961ms/step - loss: 0.3647 - accuracy: 0.8636\n",
      "Epoch 13/50\n",
      "112/112 [==============================] - 107s 953ms/step - loss: 0.3406 - accuracy: 0.8754\n",
      "Epoch 14/50\n",
      "112/112 [==============================] - 108s 961ms/step - loss: 0.3274 - accuracy: 0.8844\n",
      "Epoch 15/50\n",
      "112/112 [==============================] - 108s 961ms/step - loss: 0.3012 - accuracy: 0.8946\n",
      "Epoch 16/50\n",
      "112/112 [==============================] - 107s 956ms/step - loss: 0.2879 - accuracy: 0.9010\n",
      "Epoch 17/50\n",
      "112/112 [==============================] - 113s 1s/step - loss: 0.2743 - accuracy: 0.9044\n",
      "Epoch 18/50\n",
      "112/112 [==============================] - 108s 966ms/step - loss: 0.2559 - accuracy: 0.9104\n",
      "Epoch 19/50\n",
      "112/112 [==============================] - 107s 954ms/step - loss: 0.2296 - accuracy: 0.9282\n",
      "Epoch 20/50\n",
      "112/112 [==============================] - 107s 951ms/step - loss: 0.2276 - accuracy: 0.9237\n",
      "Epoch 21/50\n",
      "112/112 [==============================] - 105s 935ms/step - loss: 0.2151 - accuracy: 0.9277\n",
      "Epoch 22/50\n",
      "112/112 [==============================] - 106s 946ms/step - loss: 0.1945 - accuracy: 0.9370\n",
      "Epoch 23/50\n",
      "112/112 [==============================] - 106s 945ms/step - loss: 0.1725 - accuracy: 0.9432\n",
      "Epoch 24/50\n",
      "112/112 [==============================] - 106s 943ms/step - loss: 0.1796 - accuracy: 0.9418\n",
      "Epoch 25/50\n",
      "112/112 [==============================] - 109s 970ms/step - loss: 0.1777 - accuracy: 0.9394\n",
      "Epoch 26/50\n",
      "112/112 [==============================] - 107s 951ms/step - loss: 0.1580 - accuracy: 0.9457\n",
      "Epoch 27/50\n",
      "112/112 [==============================] - 107s 959ms/step - loss: 0.1584 - accuracy: 0.9504\n",
      "Epoch 28/50\n",
      "112/112 [==============================] - 106s 950ms/step - loss: 0.1454 - accuracy: 0.9543\n",
      "Epoch 29/50\n",
      "112/112 [==============================] - 106s 942ms/step - loss: 0.1519 - accuracy: 0.9522\n",
      "Epoch 30/50\n",
      "112/112 [==============================] - 106s 948ms/step - loss: 0.1375 - accuracy: 0.9553\n",
      "Epoch 31/50\n",
      "112/112 [==============================] - 107s 956ms/step - loss: 0.3386 - accuracy: 0.8649\n",
      "Epoch 32/50\n",
      "112/112 [==============================] - 106s 948ms/step - loss: 0.1922 - accuracy: 0.9341\n",
      "Epoch 33/50\n",
      "112/112 [==============================] - 105s 937ms/step - loss: 0.1489 - accuracy: 0.9512\n",
      "Epoch 34/50\n",
      "112/112 [==============================] - 105s 941ms/step - loss: 0.1358 - accuracy: 0.9544\n",
      "Epoch 35/50\n",
      "112/112 [==============================] - 104s 930ms/step - loss: 0.1163 - accuracy: 0.9615\n",
      "Epoch 36/50\n",
      "112/112 [==============================] - 106s 947ms/step - loss: 0.1222 - accuracy: 0.9613\n",
      "Epoch 37/50\n",
      "112/112 [==============================] - 105s 936ms/step - loss: 0.1172 - accuracy: 0.9619\n",
      "Epoch 38/50\n",
      "112/112 [==============================] - 106s 943ms/step - loss: 0.1144 - accuracy: 0.9630\n",
      "Epoch 39/50\n",
      "112/112 [==============================] - 106s 944ms/step - loss: 0.1024 - accuracy: 0.9681\n",
      "Epoch 40/50\n",
      "112/112 [==============================] - 104s 926ms/step - loss: 0.1061 - accuracy: 0.9650\n",
      "Epoch 41/50\n",
      "112/112 [==============================] - 105s 934ms/step - loss: 0.0973 - accuracy: 0.9686\n",
      "Epoch 42/50\n",
      "112/112 [==============================] - 104s 927ms/step - loss: 0.0957 - accuracy: 0.9692\n",
      "Epoch 43/50\n",
      "112/112 [==============================] - 103s 923ms/step - loss: 0.0943 - accuracy: 0.9685\n",
      "Epoch 44/50\n",
      "112/112 [==============================] - 106s 948ms/step - loss: 0.1126 - accuracy: 0.9634\n",
      "Epoch 45/50\n",
      "112/112 [==============================] - 106s 942ms/step - loss: 0.1029 - accuracy: 0.9677\n",
      "Epoch 46/50\n",
      "112/112 [==============================] - 105s 936ms/step - loss: 0.1018 - accuracy: 0.9658\n",
      "Epoch 47/50\n",
      "112/112 [==============================] - 104s 933ms/step - loss: 0.0979 - accuracy: 0.9688\n",
      "Epoch 48/50\n",
      "112/112 [==============================] - 104s 929ms/step - loss: 0.1012 - accuracy: 0.9657\n",
      "Epoch 49/50\n",
      "112/112 [==============================] - 104s 925ms/step - loss: 0.0990 - accuracy: 0.9696\n",
      "Epoch 50/50\n",
      "112/112 [==============================] - 104s 925ms/step - loss: 0.0944 - accuracy: 0.9689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d7145caeb0>"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(store_train[0:-500,:,:], train_Y[0:-500,:], batch_size=64, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516ee5fb",
   "metadata": {},
   "source": [
    "### evaluate the cross validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "c7249514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 5s 241ms/step - loss: 1.1093 - accuracy: 0.7980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1093380451202393, 0.7979999780654907]"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(store_train[-500:,:,:], train_Y[-500:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e2aebc",
   "metadata": {},
   "source": [
    "### test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "21cbaf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y=model.predict(store_test)\n",
    "\n",
    "test_label=[]\n",
    "\n",
    "for i in range(test_Y.shape[0]):\n",
    "    if test_Y[i,1]>=0.5:\n",
    "        test_label.append(1)\n",
    "    else:\n",
    "        test_label.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "f9530951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  target\n",
      "0   0       1\n",
      "1   2       0\n",
      "2   3       1\n",
      "3   9       0\n",
      "4  11       1\n",
      "5  12       1\n",
      "6  21       0\n",
      "7  22       0\n",
      "8  27       0\n",
      "9  29       0\n"
     ]
    }
   ],
   "source": [
    "submission=pd.DataFrame({'id': test['id'], 'target':test_label})\n",
    "print(submission.head(10))\n",
    "\n",
    "filename = 'submission_nlp_tweets.csv'\n",
    "\n",
    "submission.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743a0510",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "1056775a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd wholesale markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>we always try to bring the heavy. #metal #rt h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#africanbaze: breaking news:nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>crying out for more! set me ablaze</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>on plus side look at the sky last night it was...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Pretoria</td>\n",
       "      <td>@phdsquares #mufc they've built so much hype a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>World Wide!!</td>\n",
       "      <td>inec office in abia set ablaze - http://t.co/3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword                       location  \\\n",
       "id                                          \n",
       "48  ablaze                     Birmingham   \n",
       "49  ablaze  Est. September 2012 - Bristol   \n",
       "50  ablaze                         AFRICA   \n",
       "52  ablaze               Philadelphia, PA   \n",
       "53  ablaze                     London, UK   \n",
       "54  ablaze                       Pretoria   \n",
       "55  ablaze                   World Wide!!   \n",
       "\n",
       "                                                 text  target  \n",
       "id                                                             \n",
       "48  @bbcmtd wholesale markets ablaze http://t.co/l...       1  \n",
       "49  we always try to bring the heavy. #metal #rt h...       0  \n",
       "50  #africanbaze: breaking news:nigeria flag set a...       1  \n",
       "52                 crying out for more! set me ablaze       0  \n",
       "53  on plus side look at the sky last night it was...       0  \n",
       "54  @phdsquares #mufc they've built so much hype a...       0  \n",
       "55  inec office in abia set ablaze - http://t.co/3...       1  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[48:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbfe3b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heard about is different cities, stay safe everyone.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16741105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is up man?'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "68eb2a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how are you', 'whats up']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punc([\"how are you?\", \"what's up\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c5701a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'up', 'man']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "271340f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stopword', 'removal']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_list(remove_stop_words (nltk.word_tokenize  ('this is a stopword removal ten')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68eb59c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
